\documentclass[onecolumn, draftclsnofoot,10pt, compsoc]{IEEEtran}
\hbadness=1000 % suppress warnings
\usepackage{graphicx}
\usepackage{url}
\usepackage{setspace}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{cite}
\usepackage{geometry}

\usepackage{longtable}

\geometry{textheight=9.5in, textwidth=7in}

% 1. Fill in these details
\def \CapstoneTeamName{		Aerolyzer}
\def \CapstoneTeamNumber{		19}
\def \GroupMemberOne{			Daniel Ross}
\def \GroupMemberTwo{			Logan Wingard}
\def \CapstoneProjectName{		Aerolyzer}
\def \CapstoneSponsorPerson{		Kim Whitehall}


% 2. Uncomment the appropriate line below so that the document type works
\def \DocType{		%Problem Statement
	%Requirements Document
	%Technology Review
	%Design Document
	Progress Report
}

\newcommand{\NameSigPair}[1]{\par
	\makebox[2.75in][r]{#1} \hfil 	\makebox[3.25in]{\makebox[2.25in]{\hrulefill} \hfill		\makebox[.75in]{\hrulefill}}
	\par\vspace{-12pt} \textit{\tiny\noindent
		\makebox[2.75in]{} \hfil		\makebox[3.25in]{\makebox[2.25in][r]{Signature} \hfill	\makebox[.75in][r]{Date}}}}
% 3. If the document is not to be signed, uncomment the RENEWcommand below
\renewcommand{\NameSigPair}[1]{#1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\graphicspath{{images/}}
\begin{document}
	\begin{titlepage}
		\pagenumbering{gobble}
		\begin{singlespace}
			\centering
			\includegraphics[height=4cm,natwidth=345,natheight=435]{images/coe_v_spot1.png}
			\hfill 
			% 4. If you have a logo, use this includegraphics command to put it on the coversheet.
			%\includegraphics[height=4cm]{CompanyLogo}   
			\par\vspace{.2in}
			\centering
			\scshape{
				\huge CS Capstone \DocType \par
				{\large\today}\par
				\vspace{.5in}
				\textbf{\Huge\CapstoneProjectName}\par
				\vfill
				{\large Prepared for}\par
				{\Large\NameSigPair{\CapstoneSponsorPerson}\par}
				{\large Prepared by }\par
				Group\CapstoneTeamNumber\par
				% 5. comment out the line below this one if you do not wish to name your team
				\CapstoneTeamName\par 
				\vspace{5pt}
				{\large
					\NameSigPair{\GroupMemberOne}\par
					\NameSigPair{\GroupMemberTwo}\par
				}
				\vspace{20pt}
			}
			\begin{abstract}  
				The Aerolyzer Project aims to deliver a new source of air quality and weather information through leveraging existing weather data and image analysis algorithms.
				When complete, this open-source project shall feature a Python library that uses image classification and third-party weather APIs, displayed with an intuitive web-based user interface.
				This document outlines the software design descriptions for the Aerolyzer Library. 
			\end{abstract}     
		\end{singlespace}
	\end{titlepage}

\tableofcontents
\clearpage

\begin{singlespace}

	\section{Project Purpose}
		Monitoring atmospheric aerosols is important due to their effects on peopleâ€™s health and the atmosphere's chemical composition and radiation distribution.
		Currently delayed or inaccurate atmospheric reports complicate getting reliable local atmospheric information.
		Unfortunately, aerosols in the atmosphere are constantly changing, and current satellite, aircraft, and ground-based instruments do not simplify data enough for the average person to understand.
		There is currently no way to judge local air quality using regional aerosol data without in-depth atmospheric knowledge.
		Additionally, delayed or inaccurate atmospheric reports complicate getting reliable local atmospheric information.
		The primary objective of the Aerolyzer project is to create a tool that infers local air quality using regional weather data and image analysis.
		The major goals this presents to the project are a quick weather data retrieval, the identification of an image that can be used for color analysis, and the analysis of the colors in an image to estimate the level of aerosols.
	
	\section{Current State}
		The team is done with image metadata analysis and the weather data calls.
		Logan is currently optimizing the horizon check so the color analysis isn't performed on invalid images.
		\subsection{Logan Wingard}
			It was found that much of the image restriction functions that were believed to just a framework, had actually been written and were working with some minor errors.
			There were many errors and bugs in the pre-existing image restriction functions, some minor such as swapped variable names, and some more serious such as only allowing up to 1000x1000 pixel images instead of six times that.
			These errors have been fixed.
			There are now working image restriction functions for checking image size, image type, image resolution, phone models, for valid location data, and for if it is edited.
			The last restriction function is the horizon detection which is currently at 50\% accuracy with valid images.
			The goal for horizon detection is 66\% on valid images, which is just 16\% off from what it is now.
		\subsection{Daniel Ross}
			I completed the location data related functions for the image restriction process that occurs before calling OpenCV.
			The first, get\_coord, takes the GPS data from an image and convert that to coordinates in decimal.
			The second and third are ZIP\_to\_Coord and Coord\_to\_ZIP which call the Google Geocoding API to produce the image location in either format.
			The last is get\_sun\_position which takes an image, calls get\_coord and Coord\_to\_ZIP on that image, calls a Weather API using that ZIP code, Extracts the times of sunrise and sunset from the weather data, and finally compares the time the picture was taken to the time of the sunrise and sunset.
			
			I've written drafts of the first two functions needed for color analysis.
			The first function creates an array of RGB values that correspond to certain wavelengths of light.
			This array is created by taking a image of the visible light spectrum, that spans a range of 370nm, and scaling it so the image has a length of 370 pixels before cropping it to a height of one pixel.
			\includegraphics[height=0.02cm,natwidth=370,natheight=7]{images/Visible_Color_Spectrum_1_pixel.png}
			Then I iterate over an OpenCV image's RGB values and store them in an array.
			
			The second function compares a provided color to the array and returns the closest matching wavelength of light.
			Using the array from the first function, the function compares the subject color to each element in the array.
			Once it has the index of the closest matching color, the closest matching wavelength can be produced.
			Since the array has each element correspond to a certain wavelength in nm, all that needs to be done is add the lowest possible wavelength to the index.
			This is a rough draft of those functions.
			\begin{lstlisting}
def comparisonArray(mode)
		img = cv2.imread('VisColorSpectrum1.png')
		BGRArray = []
		HSVArray = []
		i=0
		if mode==0:
			while i<(img.shape[1]):
				BGRArray+=img[0,i]
				i+=1
			return BGRArray
		else:
			hsvimg = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
			i=0
			while i<(img.shape[1]):
				HSVArray+=hsvimg[0,i]
				i+=1
			return HSVArray

def get_wavelengthrgb((b,g,r))
	closest_r=255
	closest_g=255
	closest_b=255
	r_diff=0
	g_diff=0
	b_diff=0
	score=0
	best=0
	i=0

	while i < ARRAYLENGTH:
		if BGRArray[i][0] > b:
			b_diff = BGRArray[i][0] - b
		else:
			b_diff = b - BGRArray[i][0] 
		if BGRArray[i][1] > g:
			g_diff = BGRArray[i][1] - g
		else:
			g_diff = g - BGRArray[i][1] 
		if BGRArray[i][2] > r:
			r_diff = BGRArray[i][2] - r
		else:
			r_diff = r - BGRArray[i][2]
		if (closest_r > r_diff):
			score+=1
			closest_r=r_diff
		if (closest_b > b_diff):
			score+=1
			closest_b=b_diff
		if (closest_b > b_diff):
			score+=1
			closest_b=b_diff
			\end{lstlisting}
			
			
	\section{What's left to accomplish}
		Color analysis is the last feature we plan on adding before we get into stretch goal territory.
		\subsection{Logan Wingard}
			As mentioned in the previous section, currently the horizon detection function is at 50\% accuracy. 
			The 16 additional percent should be reachable by adjusting the current algorithm, though in an attempt to go above and beyond the 66\% goal, a test neural network will be implemented.
			This neural network will take in the histogram data of an image, and output either a 1 or 0, 1 for valid sky, and 0 for invalid.
			It is still up in the air whether a neural network is even a viable solution for this task, so much testing needs to get done before it can be implemented into our algorithm.
			After reaching the 66\% goal and converting from RGB to wavelength, a color analysis function will be written that uses information known on aerosols and how light refracts through these particles, and will get this info from the wavelengths obtained from the images.

		\subsection{Daniel Ross}
			Right now the process of accepting an image is working fairly well, but the accuracy needs to be improved to meet our original goal.
			In my current color analysis functions there are still a few things to be accomplished.
			When creating the wavelength comparison array I'm unsure as to whether RGB or HSV will be more accurate.
			On one hand RGB doesn't require any conversions for the openCV functions, but on the other hand HSV would be much easier to use in comparisons during the second function.
			\includegraphics[height=0.05cm,natwidth=370,natheight=7]{images/Visible_Color_Spectrum.png}
			
			Given the start on the color analysis produces the prominent wavelength of a certain color, the next step in color analysis is determining what that wavelength means in terms of aerosol content.
			This is the most intimidating task as far as research and correctness is concerned.
			There is possibly going to a large amount of atmospheric and physics research.
			The Rayleigh Scattering effect\cite{corfidi_2014} is the most likely route for how we're going to mathematically convert from wavelength of light to amount of aerosols in the air.


	\section{Problems}
		Since the beginning of winter term however, one member of team Aerolyzer has been removed, and much of these conflicts have been resolved.
		Towards the beginning of the term our team had a few internal issues and our client had some problems with how the project was going.
		During Fall term we worked quite a bit under the notion that the bulk of our project was going to be the development of a image classifier that identified sunrises and sunsets.
		\subsection{Logan Wingard}
			One problem that has come about was git log messiness.
			There have been too many junk commits in pull requests. 
			For example, this is one of the commits that needed to be squashed: ``Removing junk files. Please ignore''.
			The simple fix is to git rebase and go through each commit specifying which ones to pick, and which ones to fixup or squash, though this proved not to be so simple.
			Many of the commits were chained together, making it very difficult to rebase.
			The most recent issue has been getting data to train a shallow neural network that is being implemented in an attempt to increase horizon detection accuracy.
			With the amount of data needed to train a neural network, manually entering in data could be extremely time consuming, though may be the simplest way of implementing the neural network into the functions.
			The neural network that runs now takes in numpy arrays as input, though these numpy arrays work differently than the python lists I am used to. 
			Getting the data to go from integers to a numpy array of python lists of integers is proving to be a dificult task, though if succesfully implemented, would save an immense amount of time, considering the alternative would be manually entering in hundreds of data points.
		\subsection{Daniel Ross}
			At the beginning of Fall term our client informed us that this is not what she wanted for the project.
			We refocused the project, moving our development focus away from in-depth image recognition software to color analysis.
			We now have a workable version of the horizon checking function and now we're starting the color analysis like the client wants.

			In order to get the Zip Code of an image I used a call to the Google Geocoding API\cite{GoogleGeo}, which returns a json of google maps data.
			The formatting of this returned json was exceptionally difficult to work with because the API returns a dictionary containing all the results for a location search and I only needed the data from the first results.
			I spoke with our TA about it during a weekly meeting, but I ended up going to Stack Overflow to find the simplest way to navigate the API results.
			
			The other issue I encountered came from the format that EXIF data stores location data.
			EXIF stores coordinates in the traditional GPS format of Degrees,Minutes,Seconds.
			Weather Underground and Google both use decimal coordinates in their API calls.
			Converting from Degrees, Minutes, Seconds to decimal requires parsing out the original to 3 smaller conversions and then returning the sum.
	\section{Interesting Code}
		The following is an excerpt from the horizon detection function.
		It has since been updated with slightly different numbers in order to increase accuracy, but this is what is currently pushed to out master Aerolyzer github.
		It splits the image at the horizon and then splits the sky in half to analyze the histogram data in both the top and bottom half of the sky.
		These are used to further determine if the image is actually valid.
		If the image is valid, it moves onto color analysis with the bottom half of the sky.
		\begin{lstlisting}
color = ('b', 'g', 'r')
b, g, r = cv2.split(img)
dimy, dimx = img.shape[:2]

largest = [0, 0]
it = dimy / 200 #iterations = total number of rows(pixels) / 200
for i in range(dimy / 4, (dimy / 4) * 3, it):   #only looking at the middle half of the image
	ravg = (sum(r[i]) / float(len(r[i])))
	gavg = (sum(g[i]) / float(len(g[i])))
	bavg = (sum(b[i]) / float(len(b[i])))
	avg = (ravg + gavg + bavg) / 3
	pravg = (sum(r[i - it]) / float(len(r[i - it])))
	pgavg = (sum(g[i - it]) / float(len(g[i - it])))
	pbavg = (sum(b[i - it]) / float(len(b[i - it])))
	pavg = (pravg + pgavg + pbavg) / 3
	diff = pavg - avg
	if diff > largest[0]:   #only getting the largest intensity drop.
		largest = [diff,i-(it/2)]
if largest[0] >= 11:
	sky = img[0:largest[1],0:dimx]#cropping out landscape
	h1 = sky[0:(sky.shape[0] / 2),0:dimx]#top half of sky
	h2 = sky[(sky.shape[0] / 2):(sky.shape[0]), 0:dimx]#bottom half

	mask = np.zeros(h1.shape[:2], np.uint8)
	mask[0:(h1.shape[0] / 2), 0:h1.shape[1]] = 255

	for i,col in enumerate(color):
		histr = cv2.calcHist([h1], [i], mask, [255], [0, 255])
		plt.plot(histr, color = col)
		plt.xlim([0,255])

	mask = np.zeros(h2.shape[:2], np.uint8)
	mask[0:(h2.shape[0] / 2), 0:h2.shape[1]] = 255

	for i,col in enumerate(color):
		histr = cv2.calcHist([h2], [i], mask, [255], [0, 255])
		plt.plot(histr, color = col)
		plt.xlim([0, 255])
			\end{lstlisting}	
			Here are the results of this function:\\
			\includegraphics[height=4cm,natwidth=640,natheight=426]{images/horizon_uncropped.jpg}\\
			\includegraphics[height=4cm,natwidth=1281,natheight=537]{images/horizon_cropped.png}\\
			The following is the sun\_position function
			\begin{lstlisting}
def sun_position(exifdict):
	coord = get_coord(exifdict)
	wData = wunderData.get_data(str(coord[0])+","+str(coord[1]))
	sunriseTime = wData['sunrise'].split(':')
	sunsetTime = wData['sunset'].split(':')
	sunriseTarget = (int(sunriseTime[0])*60)+int(sunriseTime[1])
	sunsetTarget = (int(sunsetTime[0])*60)+int(sunsetTime[1])

	hoursTime = (str(exifdict['exif datetimeoriginal']).split(' '))[1].split(':')
	pictureTime = (int(hoursTime[0])*60)+int(hoursTime[1])+int(float(hoursTime[2])/60)

	if ((pictureTime >= (sunriseTarget - 15)) & (pictureTime <= (sunriseTarget + 30))):
		return 1
	elif ((pictureTime >= (sunsetTarget - 15)) & (pictureTime <= (sunsetTarget + 30))):
		return 2
	elif ((pictureTime > (sunsetTarget + 15))|(pictureTime < (sunriseTarget - 15))):
		return 0
	else:
		return 0
			\end{lstlisting}
\end{singlespace}
\clearpage
\bibliographystyle{IEEEtran}
\bibliography{ref}
\end{document}
